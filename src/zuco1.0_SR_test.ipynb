{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c73a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "class ZucoDataLoader:\n",
    "    def __init__(self, data_dir='../zuco_data/zuco1.0/task1-SR/Matlab files'):\n",
    "        \"\"\"Initialize ZuCo data loader for SR task.\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.subject_files = self._get_subject_files()\n",
    "        \n",
    "    def _get_subject_files(self) -> Dict[str, str]:\n",
    "        \"\"\"Get mapping of subject IDs to file paths.\"\"\"\n",
    "        subject_files = {}\n",
    "        for file_name in os.listdir(self.data_dir):\n",
    "            if file_name.endswith(\".mat\"):\n",
    "                subject_id = file_name.split('.')[0]\n",
    "                subject_files[subject_id] = os.path.join(self.data_dir, file_name)\n",
    "        return subject_files\n",
    "    \n",
    "    def get_subject_ids(self) -> List[str]:\n",
    "        \"\"\"Get list of available subject IDs.\"\"\"\n",
    "        return list(self.subject_files.keys())\n",
    "    \n",
    "    def load_subject_data(self, subject_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load raw .mat file for a subject.\"\"\"\n",
    "        file_path = self.subject_files[subject_id]\n",
    "        print(f\"Loading data from {file_path}\")\n",
    "        \n",
    "        # Load .mat file\n",
    "        data = loadmat(file_path, squeeze_me=True, struct_as_record=False)\n",
    "        return data\n",
    "    \n",
    "    def extract_word_level_features(self, subject_id: str) -> Dict[int, Dict[int, Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Extract word-level EEG and eye-tracking features.\n",
    "        \n",
    "        Returns:\n",
    "            Nested dict: {sentence_id: {word_position: {features}}}\n",
    "        \"\"\"\n",
    "        data = self.load_subject_data(subject_id)\n",
    "        sentences = data['sentenceData']\n",
    "        \n",
    "        # Initialize result structure\n",
    "        result = {}\n",
    "        \n",
    "        # Extract word-level data\n",
    "        for sent_idx, sentence in enumerate(sentences):\n",
    "            result[sent_idx] = {}\n",
    "            \n",
    "            try:\n",
    "                words = sentence.word\n",
    "                \n",
    "                for word_idx, word in enumerate(words):\n",
    "                    result[sent_idx][word_idx] = {\n",
    "                        'word': word.content if hasattr(word, 'content') else '',\n",
    "                        'sentence': sentence.content if hasattr(sentence, 'content') else '',\n",
    "                        'eeg_features': {},\n",
    "                    }\n",
    "                    \n",
    "                    # Extract EEG features\n",
    "                    for feature in ['FFD', 'TRT', 'GD', 'GPT']:\n",
    "                        for band in ['_t1', '_t2', '_a1', '_a2', '_b1', '_b2', '_g1', '_g2']:\n",
    "                            feature_name = feature + band\n",
    "                            if hasattr(word, feature_name):\n",
    "                                result[sent_idx][word_idx]['eeg_features'][feature_name] = getattr(word, feature_name)\n",
    "            \n",
    "            except (AttributeError, IndexError) as e:\n",
    "                print(f\"Error processing sentence {sent_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_sentences(self) -> Dict[int, str]:\n",
    "        \"\"\"Get all sentences in the dataset.\"\"\"\n",
    "        # Use first subject for consistency\n",
    "        subject_id = self.get_subject_ids()[0]\n",
    "        data = self.load_subject_data(subject_id)\n",
    "        sentences = data['sentenceData']\n",
    "        \n",
    "        result = {}\n",
    "        for sent_idx, sentence in enumerate(sentences):\n",
    "            try:\n",
    "                result[sent_idx] = sentence.content\n",
    "            except AttributeError:\n",
    "                continue\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697c3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "SR_DIR = '../zuco_data/zuco1.0/task1-SR/Matlab files'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009a6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available subject IDs: ['resultsZKB_SR', 'resultsZDM_SR', 'resultsZJN_SR', 'resultsZAB_SR', 'resultsZKH_SR', 'resultsZMG_SR', 'resultsZGW_SR', 'resultsZKW_SR', 'resultsZJM_SR', 'resultsZDN_SR', 'resultsZJS_SR', 'resultsZPH_SR']\n"
     ]
    }
   ],
   "source": [
    "zloader = ZucoDataLoader(SR_DIR)\n",
    "subject_ids = zloader.get_subject_ids()\n",
    "print(f\"Available subject IDs: {subject_ids}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2e7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../zuco_data/zuco1.0/task1-SR/Matlab files/resultsZAB_SR.mat\n"
     ]
    }
   ],
   "source": [
    "test_id = 'resultsZAB_SR'\n",
    "test_subj = zloader.load_subject_data('resultsZAB_SR')\n",
    "test_subj['sentenceData'][0]\n",
    "test_feat = zloader.extract_word_level_features('resultsZAB_SR')\n",
    "test_feat[0][3]['eeg_features']['FFD_t1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d1b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../zuco_data/zuco1.0/task1-SR/Matlab files/resultsZAB_SR.mat\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf1de5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words per sentence: [22, 22, 16, 5, 13, 16, 19, 11, 28, 21, 18, 7, 7, 23, 8, 21, 13, 36, 28, 25, 21, 17, 16, 6, 7, 12, 8, 10, 9, 27, 16, 23, 17, 21, 5, 26, 28, 11, 22, 17, 3, 6, 9, 6, 11, 19, 11, 7, 12, 9, 6, 29, 23, 10, 24, 31, 9, 13, 5, 12, 16, 33, 26, 7, 13, 22, 11, 14, 28, 40, 33, 29, 16, 19, 24, 14, 13, 24, 13, 9, 5, 23, 12, 13, 36, 23, 15, 18, 20, 5, 14, 26, 26, 10, 8, 21, 21, 4, 23, 16, 23, 20, 11, 28, 20, 13, 27, 15, 13, 23, 14, 10, 20, 20, 21, 20, 17, 25, 14, 28, 29, 13, 9, 14, 11, 13, 6, 32, 15, 26, 35, 13, 18, 13, 18, 3, 9, 31, 27, 16, 26, 19, 25, 13, 15, 15, 18, 24, 5, 25, 9, 18, 8, 38, 21, 14, 5, 10, 21, 16, 26, 12, 23, 24, 14, 18, 20, 20, 23, 12, 24, 7, 27, 11, 18, 8, 16, 16, 19, 18, 18, 26, 18, 7, 14, 16, 22, 16, 18, 16, 15, 13, 28, 8, 24, 18, 23, 31, 5, 30, 21, 10, 6, 26, 12, 12, 12, 10, 18, 18, 25, 24, 34, 23, 5, 31, 22, 21, 11, 15, 32, 40, 10, 26, 10, 8, 26, 15, 15, 20, 21, 24, 24, 9, 19, 24, 7, 20, 27, 23, 13, 9, 4, 35, 25, 43, 20, 13, 6, 21, 15, 19, 29, 25, 25, 11, 26, 19, 18, 18, 37, 23, 16, 9, 12, 30, 6, 10, 15, 20, 20, 28, 10, 17, 11, 37, 14, 14, 32, 34, 35, 7, 40, 4, 28, 25, 10, 18, 4, 23, 23, 5, 21, 26, 4, 18, 22, 22, 14, 8, 8, 8, 24, 25, 23, 18, 17, 16, 26, 15, 20, 15, 29, 15, 12, 25, 4, 18, 11, 23, 17, 17, 13, 15, 13, 12, 14, 14, 42, 21, 7, 22, 28, 29, 23, 5, 10, 13, 14, 3, 26, 4, 21, 18, 33, 25, 31, 27, 12, 9, 7, 18, 13, 4, 12, 7, 27, 25, 14, 16, 15, 40, 13, 10, 26, 14, 26, 8, 20, 24, 18, 11, 12, 20, 14, 12, 23, 15, 20, 23, 18, 35, 37, 13, 5, 18, 21, 32, 18, 7, 25, 18, 10, 8, 15, 12, 24, 13, 16, 9]\n"
     ]
    }
   ],
   "source": [
    "test_words_per_sentence = [test_feat[i].keys().__len__() for i in range(len(test_feat))]\n",
    "print(f\"Number of words per sentence: {words_per_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b12ebd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../zuco_data/zuco1.0/task1-SR/Matlab files/resultsZKB_SR.mat\n"
     ]
    }
   ],
   "source": [
    "sentences = zloader.get_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "179bcb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 400\n",
      "Average words per sentence: 17.8225\n"
     ]
    }
   ],
   "source": [
    "words_per_sentence = [len(sentences[i].split()) for i in range(len(sentences))]\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "print(f\"Average words per sentence: {np.mean(words_per_sentence)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f35b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "not any(np.array(words_per_sentence) - np.array(test_words_per_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc4d3d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feat[0][3]['eeg_features']['FFD_t1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d431b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
